{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-23T12:03:15.858543Z",
     "start_time": "2025-12-23T12:03:14.363623Z"
    }
   },
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus.reader import TITLE\n",
    "\n",
    "df = pd.read_csv(\"../data/recipes.csv\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "base_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "units = {\n",
    "    \"cup\", \"cups\", \"ts\", \"tsp\", \"teaspoon\", \"tbsp\", \"tablespoon\",\n",
    "    \"oz\", \"ounce\", \"lb\", \"pound\", \"g\", \"gram\", \"kg\", \"ml\", \"l\", \"liter\",\n",
    "    \"pint\", \"quart\", \"gallon\", \"inch\", \"diameter\",\n",
    "    \"pinch\", \"dash\", \"clove\", \"sprig\", \"stick\", \"head\", \"bunch\",\n",
    "    \"slice\", \"piece\", \"chunk\", \"part\", \"portion\",\n",
    "    \"can\", \"canned\", \"jar\", \"package\", \"packet\", \"box\", \"bag\", \"bottle\"\n",
    "}\n",
    "\n",
    "adjectives = {\n",
    "    \"large\", \"small\", \"medium\", \"tiny\", \"huge\", \"whole\",\n",
    "    \"hot\", \"cold\", \"warm\", \"boiling\", \"room\", \"temperature\",\n",
    "    \"fresh\", \"dry\", \"dried\", \"frozen\", \"thawed\",\n",
    "    \"organic\", \"kosher\", \"virgin\", \"extra\",\n",
    "    \"lean\", \"fat\",\n",
    "    \"good\", \"best\", \"quality\", \"fine\", \"finely\", \"coarsely\",\n",
    "    \"sturdy\", \"attached\", \"flat\", \"rotten\", \"ripe\", \"ripened\",\n",
    "    \"storebought\", \"homemade\", \"preferably\",\n",
    "    \"new\", \"old\", \"sharp\", \"mild\", \"soft\", \"hard\"\n",
    "}\n",
    "\n",
    "methods = {\n",
    "    \"chopped\", \"diced\", \"minced\", \"sliced\", \"grated\", \"peeled\", \"cored\", \"seeded\",\n",
    "    \"shredded\", \"crushed\", \"mashed\", \"ground\", \"beaten\", \"whisked\", \"stirred\",\n",
    "    \"cooked\", \"roasted\", \"grilled\", \"baked\", \"fried\", \"boiled\", \"steamed\", \"poached\",\n",
    "    \"melted\", \"softened\", \"divided\", \"separated\", \"removed\", \"discarded\",\n",
    "    \"patted\", \"drained\", \"rinsed\", \"washed\", \"stuffed\", \"dressed\", \"trimmed\",\n",
    "    \"boneless\", \"skinless\", \"skin\", \"cured\", \"preserved\", \"pitted\", \"halved\", \"quartered\",\n",
    "    \"cut\", \"torn\", \"broken\", \"carcass\"\n",
    "}\n",
    "\n",
    "context_fillers = {\n",
    "    \"optional\", \"garnish\", \"serving\", \"taste\", \"accompaniment\",\n",
    "    \"plus\", \"more\", \"total\", \"about\", \"approx\", \"exceed\",\n",
    "    \"seasoning\", \"preparation\", \"finish\", \"finishing\", \"topping\"\n",
    "}\n",
    "\n",
    "colors = {\n",
    "     \"black\", \"green\", \"yellow\", \"blue\", \"brown\", \"orange\", \"pink\"\n",
    "}\n",
    "\n",
    "brands_trash = {\n",
    "    \"lindt\", \"perugina\", \"ghirardelli\", \"gala\", \"lady\", \"fed\", \"grass\"\n",
    "}\n",
    "\n",
    "smart_stopwords = base_stopwords.union(units, adjectives, methods, context_fillers, colors, brands_trash)\n",
    "\n",
    "\n",
    "def preprocess_ingredients_smart(ingredients):\n",
    "\n",
    "    try:\n",
    "        if isinstance(ingredients, str):\n",
    "            ingredients = ingredients.strip(\"[]\").replace(\"'\", \"\").split(', ')\n",
    "        elif isinstance(ingredients, list):\n",
    "            ingredients = ingredients\n",
    "        else:\n",
    "            return []\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "    cleaned_tokens = []\n",
    "    for item in ingredients:\n",
    "        text = item.lower()\n",
    "        text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "        #remove numbers, fraction symbols\n",
    "        text = re.sub(r'[\\d½¾¼⅓⅔⅛⅜⅝⅞]+', '', text)\n",
    "        #remove everything that is not a letter\n",
    "        text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        #tags for the part of speech of the word\n",
    "        tagged_tokens = nltk.pos_tag(tokens)\n",
    "        #check if the word is a Nomen (NN) or Adjective (JJ) and lemmatize it, if not in stopwords then append to the array\n",
    "        for word, tag in tagged_tokens:\n",
    "            if tag.startswith('NN') or tag.startswith('JJ'):\n",
    "                lemma = lemmatizer.lemmatize(word)\n",
    "                if lemma not in smart_stopwords and len(lemma) > 2:\n",
    "                    cleaned_tokens.append(lemma)\n",
    "\n",
    "    return cleaned_tokens"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T12:03:16.093056Z",
     "start_time": "2025-12-23T12:03:15.864518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    df = pd.read_csv(\"../data/recipes.csv\")\n",
    "    print(\"✅ Daten erfolgreich geladen!\")\n",
    "    print(f\"Datensatz hat {df.shape[0]} Zeilen und {df.shape[1]} Spalten.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"FEHLER: Die CSV-Datei wurde nicht gefunden.\")"
   ],
   "id": "fddd5c5931ecd57",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Daten erfolgreich geladen!\n",
      "Datensatz hat 13501 Zeilen und 6 Spalten.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T12:03:52.925370Z",
     "start_time": "2025-12-23T12:03:16.283954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#nltk.download('punkt_tab')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "\n",
    "#part of speech tagging\n",
    "print(\"start preprocessing with pos tagging...\")\n",
    "\n",
    "df['ingredients_smart'] = df['Cleaned_Ingredients'].apply(preprocess_ingredients_smart)\n",
    "\n",
    "\n",
    "print(\"\\n--- results  ---\")\n",
    "for i in range(3):\n",
    "    print(f\"RAW: {df['Ingredients'].iloc[i][:100]}...\")\n",
    "    print(f\"SMART: {df['ingredients_smart'].iloc[i]}\")\n",
    "    print(\"-\" * 20)"
   ],
   "id": "41351e05f88e0d1d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start preprocessing with pos tagging...\n",
      "\n",
      "--- results  ---\n",
      "RAW: ['1 (3½–4-lb.) whole chicken', '2¾ tsp. kosher salt, divided, plus more', '2 small acorn squash (abo...\n",
      "SMART: ['chicken', 'salt', 'acorn', 'squash', 'sage', 'rosemary', 'butter', 'allspice', 'red', 'pepper', 'flake', 'pepper', 'white', 'bread', 'apple', 'olive', 'oil', 'red', 'onion', 'apple', 'cider', 'vinegar', 'white', 'miso', 'flour', 'butter', 'white', 'wine', 'chicken', 'broth', 'white', 'miso', 'salt', 'pepper']\n",
      "--------------------\n",
      "RAW: ['2 large egg whites', '1 pound new potatoes (about 1 inch in diameter)', '2 teaspoons kosher salt',...\n",
      "SMART: ['egg', 'white', 'potato', 'salt', 'pepper', 'rosemary', 'thyme', 'parsley']\n",
      "--------------------\n",
      "RAW: ['1 cup evaporated milk', '1 cup whole milk', '1 tsp. garlic powder', '1 tsp. onion powder', '1 tsp....\n",
      "SMART: ['milk', 'milk', 'garlic', 'powder', 'onion', 'powder', 'paprika', 'pepper', 'salt', 'cheddar', 'full', 'cream', 'cheese', 'elbow', 'macaroni']\n",
      "--------------------\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T12:03:53.475804Z",
     "start_time": "2025-12-23T12:03:52.943500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models import Phrases\n",
    "\n",
    "print(\"start training with bigrams\")\n",
    "\n",
    "phrases_model_smart = Phrases(df['ingredients_smart'], min_count=10, threshold=0.4, scoring=\"npmi\")\n",
    "bigram_model = Phraser(phrases_model_smart)\n",
    "\n",
    "\n",
    "def apply_bigrams(tokens):\n",
    "    return bigram_model[tokens]\n",
    "\n",
    "df['ingredients_bigrams'] = df['ingredients_smart'].apply(apply_bigrams)\n",
    "\n",
    "\n",
    "print(\"\\n--- Beispiel für erkannte Bigramme ---\")\n",
    "for i in range(len(df)):\n",
    "    if df['ingredients_smart'].iloc[i] != df['ingredients_bigrams'].iloc[i]:\n",
    "        print(f\"Vorher: {df['Ingredients'].iloc[i]}\")\n",
    "        print(f\"Nachher: {df['ingredients_bigrams'].iloc[i]}\")\n",
    "        print(\"-\" * 30)\n",
    "        break"
   ],
   "id": "e74c42f5b7f5342f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training with bigrams\n",
      "\n",
      "--- Beispiel für erkannte Bigramme ---\n",
      "Vorher: ['1 (3½–4-lb.) whole chicken', '2¾ tsp. kosher salt, divided, plus more', '2 small acorn squash (about 3 lb. total)', '2 Tbsp. finely chopped sage', '1 Tbsp. finely chopped rosemary', '6 Tbsp. unsalted butter, melted, plus 3 Tbsp. room temperature', '¼ tsp. ground allspice', 'Pinch of crushed red pepper flakes', 'Freshly ground black pepper', '⅓ loaf good-quality sturdy white bread, torn into 1\" pieces (about 2½ cups)', '2 medium apples (such as Gala or Pink Lady; about 14 oz. total), cored, cut into 1\" pieces', '2 Tbsp. extra-virgin olive oil', '½ small red onion, thinly sliced', '3 Tbsp. apple cider vinegar', '1 Tbsp. white miso', '¼ cup all-purpose flour', '2 Tbsp. unsalted butter, room temperature', '¼ cup dry white wine', '2 cups unsalted chicken broth', '2 tsp. white miso', 'Kosher salt, freshly ground pepper']\n",
      "Nachher: ['chicken', 'salt', 'acorn_squash', 'sage', 'rosemary', 'butter', 'allspice', 'red_pepper', 'flake', 'pepper', 'white', 'bread', 'apple', 'olive_oil', 'red_onion', 'apple_cider', 'vinegar', 'white_miso', 'flour', 'butter', 'white_wine', 'chicken_broth', 'white_miso', 'salt_pepper']\n",
      "------------------------------\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T12:03:55.178542Z",
     "start_time": "2025-12-23T12:03:53.493188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "print(\"start word2vec training...\")\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "model = Word2Vec(df['ingredients_bigrams'],workers=cores-4, vector_size=150, window=10, min_count=5, sg=1)\n",
    "\n",
    "os.makedirs(\"../data/models\", exist_ok=True)\n",
    "\n",
    "model_path = \"../data/models/word2vec.model\"\n",
    "model.save(model_path)\n",
    "\n",
    "print(f\"model saved under: {model_path}\")\n",
    "\n",
    "\n",
    "ingredient_vectors = {}\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    ingredients = row['ingredients_bigrams']\n",
    "\n",
    "#iterate over the tokenised bigrams and check if the word2vec-model (model.wv=word vector) recognizes the word -> if yes take the numbers (wv) and put it in the array [vectors]\n",
    "    vectors = []\n",
    "    for word in ingredients:\n",
    "        if word in model.wv:\n",
    "            vectors.append(model.wv[word])\n",
    "\n",
    "#calculate the mean\n",
    "    if vectors:\n",
    "        avg_vector = np.mean(vectors, axis=0)\n",
    "        ingredient_vectors[i] = avg_vector\n",
    "\n",
    "\n",
    "\n",
    "folder_path = \"../data/models\"\n",
    "pkl_path = os.path.join(folder_path, \"ingredient_vectors.pkl\")\n",
    "\n",
    "\n",
    "with open(pkl_path, \"wb\") as f:\n",
    "    pickle.dump(ingredient_vectors, f)\n",
    "    print(f\"model saved under: {pkl_path}\")\n"
   ],
   "id": "e023b4ce3b704e20",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start word2vec training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved under: ../data/models/word2vec.model\n",
      "model saved under: ../data/models\\ingredient_vectors.pkl\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T12:03:55.256340Z",
     "start_time": "2025-12-23T12:03:55.195715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loaded_model_smart = Word2Vec.load(model_path)\n",
    "\n",
    "def check_smart(term, model):\n",
    "        similar = model.wv.most_similar(term, topn=5)\n",
    "        print(f\"\\n Similarity or Alternative to '{term}':\")\n",
    "        for item, score in similar:\n",
    "            print(f\"  -> {item} ({score:.2f})\")\n",
    "\n",
    "\n",
    "check_smart(\"chicken\", loaded_model_smart)\n",
    "check_smart(\"beef\", loaded_model_smart)\n",
    "check_smart(\"chocolate\", loaded_model_smart)\n",
    "check_smart(\"spaghetti\", loaded_model_smart)\n",
    "check_smart(\"tomato\", loaded_model_smart)\n",
    "check_smart(\"oil\", loaded_model_smart)\n",
    "#check_smart(\"cook\", loaded_model_smart)\n"
   ],
   "id": "8a68f6a557f0cada",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Similarity or Alternative to 'chicken':\n",
      "  -> drumstick (0.84)\n",
      "  -> wing (0.83)\n",
      "  -> leg_thigh (0.81)\n",
      "  -> backbone (0.81)\n",
      "  -> breast (0.81)\n",
      "\n",
      " Similarity or Alternative to 'beef':\n",
      "  -> beef_chuck (0.82)\n",
      "  -> roast (0.81)\n",
      "  -> meaty (0.79)\n",
      "  -> sirloin (0.77)\n",
      "  -> chuck (0.77)\n",
      "\n",
      " Similarity or Alternative to 'chocolate':\n",
      "  -> chocolate_chip (0.93)\n",
      "  -> bittersweet_semisweet (0.91)\n",
      "  -> semisweet_bittersweet (0.89)\n",
      "  -> bittersweet_chocolate (0.89)\n",
      "  -> bar (0.89)\n",
      "\n",
      " Similarity or Alternative to 'spaghetti':\n",
      "  -> rigatoni (0.92)\n",
      "  -> bucatini (0.92)\n",
      "  -> linguine (0.92)\n",
      "  -> orecchiette (0.92)\n",
      "  -> broccoli_rabe (0.92)\n",
      "\n",
      " Similarity or Alternative to 'tomato':\n",
      "  -> plum_tomato (0.81)\n",
      "  -> oregano (0.77)\n",
      "  -> rom_tomato (0.76)\n",
      "  -> pimiento (0.76)\n",
      "  -> undrained (0.74)\n",
      "\n",
      " Similarity or Alternative to 'oil':\n",
      "  -> neutral (0.70)\n",
      "  -> canola (0.69)\n",
      "  -> grapeseed (0.67)\n",
      "  -> stir (0.66)\n",
      "  -> sunflower (0.66)\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
