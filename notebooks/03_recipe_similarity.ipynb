{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-23T12:05:51.825977Z",
     "start_time": "2025-12-23T12:05:50.188158Z"
    }
   },
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import pickle\n",
    "import multiprocessing\n",
    "import re\n",
    "from nltk import WordNetLemmatizer\n",
    "df = pd.read_csv(\"../data/recipes.csv\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "print(df.columns)\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "def preprocess_instructions(instructions):\n",
    "    try:  #remove the square brackets and quotation marks, separate at commas\n",
    "        instructions = instructions.strip(\"[]\").replace(\"'\", \"\").split(', ')\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "    cleaned_tokens = []\n",
    "    for item in instructions:\n",
    "        text = item.lower()  #lowercase\n",
    "        text = re.sub(r'[\\d½¾¼⅓⅔⅛⅜⅝⅞]+', '', text)  #remove numbers, fraction symbols\n",
    "        text = re.sub(r'[^a-z\\s]', ' ', text)  #replace everything that is not a letter with a space\n",
    "\n",
    "        tokens = text.split()\n",
    "        for token in tokens:\n",
    "            lemma = lemmatizer.lemmatize(token)\n",
    "            if len(lemma) > 2:\n",
    "                cleaned_tokens.append(lemma)\n",
    "    return cleaned_tokens\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'Title', 'Ingredients', 'Instructions', 'Image_Name',\n",
      "       'Cleaned_Ingredients'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T12:06:00.986405Z",
     "start_time": "2025-12-23T12:05:51.835006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "print(\"start preprocessing\")\n",
    "\n",
    "#create processed instructions row in dataset (saved in ram)\n",
    "df['instructions_processed'] = df['Instructions'].apply(preprocess_instructions)\n",
    "\n",
    "print(\"finished preprocessing for recipes\")\n",
    "\n",
    "#reset index, for id´s even is we got the C1 in the dataset\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "#gensim needs the taggedoc, doesn´t work with the id or C1 from pandas\n",
    "print(\"create taggeddocuments\")\n",
    "tagged_data = [\n",
    "    TaggedDocument(words=row['instructions_processed'], tags=[str(i)])\n",
    "    for i, row in df.iterrows()\n",
    "]\n",
    "\n",
    "print(f\"   -> {len(tagged_data)} recipes prepared\")\n"
   ],
   "id": "e84d715ef8cf78a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start preprocessing\n",
      "finished preprocessing for recipes\n",
      "create taggeddocuments\n",
      "   -> 13501 recipes prepared\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T12:06:01.141477Z",
     "start_time": "2025-12-23T12:06:01.126473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "print(\"Cores:\" , cores)\n",
    "print(df.columns)\n",
    "\n",
    "print(\"Initialize Doc2Vec Model\")\n",
    "model = Doc2Vec(vector_size=150, window=5, min_count=5, dm=1, epochs=10, workers=cores-4)\n",
    "\n",
    "\n"
   ],
   "id": "1d663907e3ef8e1c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cores: 20\n",
      "Index(['Title', 'Ingredients', 'Instructions', 'Image_Name',\n",
      "       'Cleaned_Ingredients', 'instructions_processed'],\n",
      "      dtype='object')\n",
      "Initialize Doc2Vec Model\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T12:06:12.594188Z",
     "start_time": "2025-12-23T12:06:01.157738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#count the words, create structure\n",
    "model.build_vocab(tagged_data)\n",
    "print(\"created vocabulary\")\n",
    "\n",
    "print(\"start training\")\n",
    "#train with the tagged data, recipe counts and the iterations\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n"
   ],
   "id": "8c7d470c74ab442d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created vocabulary\n",
      "start training\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T12:06:12.673467Z",
     "start_time": "2025-12-23T12:06:12.611200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "folder_path = \"../data/models\"\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "\n",
    "recipe_model_path = os.path.join(folder_path, \"doc2vec.model\")\n",
    "model.save(recipe_model_path)\n",
    "print(f\"model saved to {recipe_model_path}\")\n",
    "\n",
    "\n",
    "\n",
    "pkl_path = os.path.join(folder_path, \"recipe_vectors.pkl\")\n",
    "\n",
    "recipe_vectors = {\n",
    "    i: model.dv[str(i)]\n",
    "    for i in range(len(df))\n",
    "}\n",
    "\n",
    "\n",
    "#wb -> write binary\n",
    "with open(pkl_path, 'wb') as f:\n",
    "    pickle.dump(recipe_vectors, f)\n",
    "\n",
    "    print(f\"saved vector database{pkl_path}\")"
   ],
   "id": "3de239b9376210c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved to ../data/models\\doc2vec.model\n",
      "saved vector database../data/models\\recipe_vectors.pkl\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T12:06:12.720273Z",
     "start_time": "2025-12-23T12:06:12.690681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_id = 100\n",
    "\n",
    "title_original = df.iloc[test_id]['Title']\n",
    "\n",
    "print(f\"search similarity #{test_id}: '{title_original}'\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "sims = model.dv.most_similar(str(test_id), topn=5)\n",
    "\n",
    "for doc_id, score in sims:\n",
    "    idx = int(doc_id)\n",
    "    title = df.iloc[idx]['Title']\n",
    "    print(f\"Score: {score:.3f} | ID: {idx} | {title}\")"
   ],
   "id": "60bfb7b1ba53424a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search similarity #100: 'Stuffed Eggplants and Zucchini in a Rich Tomato Sauce (Baatingan w Kusaa Bil Banadoura)'\n",
      "--------------------------------------------------\n",
      "Score: 0.602 | ID: 7652 | Meatballs: The Spuntino Way\n",
      "Score: 0.593 | ID: 769 | Kimchi and Miso Noodle Soup\n",
      "Score: 0.580 | ID: 5062 | Turkey Breast Stuffed with Italian Sausage and Marsala-Steeped Cranberries\n",
      "Score: 0.565 | ID: 101 | Chicken Meatballs With Molokhieh, Garlic, and Cilantro\n",
      "Score: 0.558 | ID: 2866 | Jollof Rice\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T12:06:12.782826Z",
     "start_time": "2025-12-23T12:06:12.737518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(\"load recipe vectors\")\n",
    "folder_path = \"../data/models\"\n",
    "file_path = os.path.join(folder_path, \"recipe_vectors.pkl\")\n",
    "\n",
    "#rb -> read binary\n",
    "if os.path.exists(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        loaded_vectors = pickle.load(f)\n",
    "\n",
    "        print(f\"loaded vectors from {file_path}\")\n",
    "\n",
    "\n",
    "    test_idx = 100\n",
    "    vector = loaded_vectors[test_idx]\n",
    "\n",
    "    print(f\"Check für Rezept ID {test_idx}:\")\n",
    "    print(f\" - Typ: {type(vector)}\")\n",
    "    print(f\" - Form (Dimension): {vector.shape}\")\n",
    "    print(f\" - Erste 5 Werte: {vector[:5]}\")\n",
    "\n",
    "\n"
   ],
   "id": "3866ef0a41fd365f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load recipe vectors\n",
      "loaded vectors from ../data/models\\recipe_vectors.pkl\n",
      "Check für Rezept ID 100:\n",
      " - Typ: <class 'numpy.ndarray'>\n",
      " - Form (Dimension): (150,)\n",
      " - Erste 5 Werte: [-0.6231667   0.4053907  -0.41491544  0.02189164 -0.93432814]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T12:06:13.097610Z",
     "start_time": "2025-12-23T12:06:12.831220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/recipes.csv\")\n",
    "\n",
    "rows = ['Title', 'Ingredients', 'Instructions', 'Image_Name']\n",
    "df_metadata = df[rows].copy()\n",
    "\n",
    "path = \"../data/models/metadata.pkl\"\n",
    "\n",
    "\n",
    "with open(path, 'wb') as f:\n",
    "    pickle.dump(df_metadata, f)\n",
    "\n",
    "    print(\"loaded metadata\")\n",
    "\n",
    "    print(df_metadata.head())"
   ],
   "id": "40a59821bf987521",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded metadata\n",
      "                                               Title  \\\n",
      "0  Miso-Butter Roast Chicken With Acorn Squash Pa...   \n",
      "1                    Crispy Salt and Pepper Potatoes   \n",
      "2                        Thanksgiving Mac and Cheese   \n",
      "3                 Italian Sausage and Bread Stuffing   \n",
      "4                                       Newton's Law   \n",
      "\n",
      "                                         Ingredients  \\\n",
      "0  ['1 (3½–4-lb.) whole chicken', '2¾ tsp. kosher...   \n",
      "1  ['2 large egg whites', '1 pound new potatoes (...   \n",
      "2  ['1 cup evaporated milk', '1 cup whole milk', ...   \n",
      "3  ['1 (¾- to 1-pound) round Italian loaf, cut in...   \n",
      "4  ['1 teaspoon dark brown sugar', '1 teaspoon ho...   \n",
      "\n",
      "                                        Instructions  \\\n",
      "0  Pat chicken dry with paper towels, season all ...   \n",
      "1  Preheat oven to 400°F and line a rimmed baking...   \n",
      "2  Place a rack in middle of oven; preheat to 400...   \n",
      "3  Preheat oven to 350°F with rack in middle. Gen...   \n",
      "4  Stir together brown sugar and hot water in a c...   \n",
      "\n",
      "                                          Image_Name  \n",
      "0  miso-butter-roast-chicken-acorn-squash-panzanella  \n",
      "1         crispy-salt-and-pepper-potatoes-dan-kluger  \n",
      "2         thanksgiving-mac-and-cheese-erick-williams  \n",
      "3          italian-sausage-and-bread-stuffing-240559  \n",
      "4                 newtons-law-apple-bourbon-cocktail  \n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
